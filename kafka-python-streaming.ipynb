{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45213727-d84a-4e6f-a460-38a5a67c5068",
   "metadata": {},
   "source": [
    "# Streaming Youtube Comments Using kafka-pyspark Integration\n",
    "\n",
    "This project uses youtube comments scraped off of the youtube API and saved ina  json file to explore data streaming using a kafka-pyspark integration. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed90915-3f52-4063-9dff-38db6a916839",
   "metadata": {},
   "source": [
    "## A. Set up the docker environment, open Jupyter notebook\n",
    "After downloading and running Docker Desktop, we used the `docker-compose.yaml` source file from [this github repository](https://github.com/subhamkharwal/docker-images) to create the required docker containers for our kafka-pyspark pipeline.\n",
    "\n",
    "First, we compose the docker containers in the cmd, using the command:\n",
    "\n",
    "`docker compose up`\n",
    "\n",
    "Once the containers are up and running, we can open this jupyter notebook using the link: \n",
    "\n",
    "`localhost:8888`\n",
    "\n",
    "## B. Create a Spark Session\n",
    "We then create a spark session, and download the necessary jar packages for running kafka. In this case, the jar packages are: \n",
    "\n",
    "`'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90ae9a52-c50b-495d-b29d-ddbfd916a34a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://f4ca82ca20bb:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Streaming from Kafka</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f09da7e6ad0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Streaming from Kafka\")\n",
    "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0')\n",
    "    .config(\"spark.sql.shuffle.partitions\", 4)\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "#check if spark session is active\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6cc95a-d100-4db9-aa4d-094b31aece1d",
   "metadata": {},
   "source": [
    "The spark session is active, as the spark UI is visible. We can use the link \"localhost:4040\" to access the Spark UI and monitor the jobs in progress in the application. \n",
    "\n",
    "## C. Configure the kafka container\n",
    "Next, in command prompt, we connect to the kafka container in the cmd using the following command:\n",
    "\n",
    "`docker exec -it abd99b4a9cf2a00a745dc72aa2358b375e09a38f2506502650fc67290577f466 /bin/bash`\n",
    "\n",
    "We will then create a new topic known as *comments-data*, using the command:\n",
    "\n",
    "`kafka-topics --create --topic tweet-data --bootstrap-server localhost:29092`\n",
    "\n",
    "To view if the topic has been created, we use the command: \n",
    "\n",
    "`kafka-topics --list --bootstrap-server localhost:29092`\n",
    "\n",
    "We then pase some data into the topic, using the command:\n",
    "\n",
    "`kafka-console-producer --topic comments-data --bootstrap-server localhost:29092`\n",
    "\n",
    "## D. Create and configure Kafka_df\n",
    "This is the dataframe that will be storing the streaming comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ec540d3-a423-4d64-a85b-fb6e5d27d1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the kafka_df to read from kafka\n",
    "\n",
    "kafka_df = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"ed-kafka:29092\")\n",
    "    .option(\"subscribe\", \"comments-data\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0324224-09b4-4b7f-822e-4a194b207015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#View schema for raw kafka_df\n",
    "kafka_df.printSchema()\n",
    "#kafka_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2401daf3-60ea-420c-be8b-0d44956235cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse value from binary to string into kafka_json_df\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "kafka_json_df = kafka_df.withColumn(\"value\", expr(\"cast(value as string)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca025349-cd8d-4af5-88b5-b9df62c67d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-------------+---------+------+--------------------+-------------+\n",
      "| key|               value|        topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+-------------+---------+------+--------------------+-------------+\n",
      "|null|{\"author\": \"@bish...|comments-data|        0|     0|2024-04-04 03:35:...|            0|\n",
      "|null|                    |comments-data|        0|     1|2024-04-04 03:35:...|            0|\n",
      "|null|{\"author\": \"@bish...|comments-data|        0|     2|2024-04-04 03:36:...|            0|\n",
      "+----+--------------------+-------------+---------+------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#kafka_json_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95097e40-64e9-4256-a312-4c3a761ea0e9",
   "metadata": {},
   "source": [
    "We then specify the schema of the comments: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6bb2014-2629-4e36-a468-71cf7e84674b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, StructField, StructType, ArrayType, LongType\n",
    "\n",
    "json_schema = StructType([\n",
    "    StructField('author', StringType(), True),\n",
    "    StructField('published_at', StringType(), True),\n",
    "    StructField('updated_at', StringType(), True),\n",
    "    StructField('like_count', StringType(), True),\n",
    "    StructField('text', StringType(), True)\n",
    "])\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d46a99cb-ada4-4c1b-aa65-40b5593f0236",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the schema to payload to read data\n",
    "from pyspark.sql.functions import from_json, col\n",
    "\n",
    "streaming_df = kafka_json_df.withColumn(\"values_json\", from_json(col(\"value\"), json_schema)).selectExpr(\"values_json.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e690b00-8d63-422b-9ea7-0d4056cbcd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- published_at: string (nullable = true)\n",
      " |-- updated_at: string (nullable = true)\n",
      " |-- like_count: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#To the shcema of the data, place a sample json file and change readstream to read\n",
    "streaming_df.printSchema()\n",
    "#streaming_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ab4b2f-fd25-4674-8524-254975c61d91",
   "metadata": {},
   "source": [
    "## Post the Streaming Comments to kafka\n",
    "We then run the following commands in the command prompt to run the post_to_kafka.py\n",
    "\n",
    "`pip install kafka-python`\n",
    "\n",
    "`python /home/jovyan/kafka_spark_streaming/utils/post_to_kafka.py`\n",
    "\n",
    "## Write the streaming comments to the Kafka Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc807b2-ec52-4aea-9d73-82b2d7364634",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the output to console sink to check the output- batch mode\n",
    "(streaming_df\n",
    ".writeStream\n",
    ".format(\"console\")\n",
    ".outputMode('append')\n",
    ".trigger(processingTime = '10 seconds')\n",
    ".option(\"checkpointLocation\", \"checkpoint_dir_kafka2\")\n",
    ".start()\n",
    ".awaitTermination())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
